{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22aaa206-ee0c-4e76-8952-39c36b72e150",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gumpun\\AppData\\Local\\Temp\\ipykernel_15816\\196635466.py:8: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.optim import AdamW\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a179119-4aab-4e82-9b9a-9bd87463eb79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def file2list(fname):\n",
    "    with open(fname, encoding='utf-8') as fp:  # Specify the encoding\n",
    "        lines = fp.readlines()\n",
    "        return [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d6c97e4-88ef-4dbf-9165-236400dd4fe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos_text = (\"C:/Users/Gumpun/Sentiment-Analysis-System-for-Consumer-Products/DataReviewProductThai/Positive.csv\")\n",
    "neg_text = (\"C:/Users/Gumpun/Sentiment-Analysis-System-for-Consumer-Products/DataReviewProductThai/Negative.csv\")\n",
    "neu_text = (\"C:/Users/Gumpun/Sentiment-Analysis-System-for-Consumer-Products/DataReviewProductThai/Neutrally.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e970c4b-3027-4b89-8174-7dbb9c85221b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Message</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>Ronnarong P.</td>\n",
       "      <td>‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏ú‡πâ‡∏≤‡∏ö‡∏≤‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ñ‡∏≤‡∏î</td>\n",
       "      <td>Neutrally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>0***7</td>\n",
       "      <td>‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏π‡∏•‡∏≥‡πÇ‡∏û‡∏á‡∏Ñ‡πà‡∏∞‡πÅ‡∏ï‡πà‡∏Å‡πá‡πÇ‡∏≠‡πÄ‡∏Ñ‡∏≠‡∏¢‡∏π‡πà</td>\n",
       "      <td>Neutrally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>v***n</td>\n",
       "      <td>‡∏ö‡∏≤‡∏á‡∏°‡∏≤‡∏Å‡∏Ñ‡∏∞</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>‡πÄ***.</td>\n",
       "      <td>‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏õ‡∏Å</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Pimpida P.</td>\n",
       "      <td>‡∏î‡∏µ‡∏°‡∏≤‡∏Å</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>T***.</td>\n",
       "      <td>‡∏î‡∏µ</td>\n",
       "      <td>Neutrally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>tananya</td>\n",
       "      <td>‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å‡∏™‡∏±‡πà‡∏á‡∏°‡∏≤‡∏≠‡∏±‡∏ô ‡∏≠‡∏±‡∏ô‡∏ô‡∏∂‡∏á‡πÑ‡∏î‡πâ‡∏¢‡∏¥‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô ‡∏≠‡∏µ‡∏Å‡∏≠‡∏±‡∏ô...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>WANIDAü§çü¶ã</td>\n",
       "      <td>‡∏Ñ‡∏ô‡∏•‡∏∞‡∏™‡∏µ‡∏Å‡∏∞‡∏ó‡∏µ‡πà‡∏™‡∏±‡πà‡∏á‡πÄ‡∏•‡∏¢</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1317</th>\n",
       "      <td>‡∏à***.</td>\n",
       "      <td>‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏£‡πâ‡∏≤‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å ‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏™‡πà‡∏á‡∏Ç‡∏≠‡∏á‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å‡πÜ ‡πÅ‡∏¢‡πà</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>‡∏£***.</td>\n",
       "      <td>‡∏ó‡∏≤‡∏á‡∏£‡πâ‡∏≤‡∏ô‡πÄ‡πÄ‡∏û‡πá‡∏Ñ‡∏Ç‡∏≠‡∏á‡∏°‡∏≤‡∏î‡∏µ‡∏Ñ‡πà‡∏∞ ‡∏Å‡∏≤‡∏£‡∏Ç‡∏ô‡∏™‡πà‡∏á‡∏î‡∏µ ‡∏û‡∏ô‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏™‡πà‡∏á‡∏°‡∏µ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              User                                            Message  \\\n",
       "217   Ronnarong P.                                 ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏ú‡πâ‡∏≤‡∏ö‡∏≤‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ñ‡∏≤‡∏î   \n",
       "1419         0***7                       ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏π‡∏•‡∏≥‡πÇ‡∏û‡∏á‡∏Ñ‡πà‡∏∞‡πÅ‡∏ï‡πà‡∏Å‡πá‡πÇ‡∏≠‡πÄ‡∏Ñ‡∏≠‡∏¢‡∏π‡πà   \n",
       "8            v***n                                           ‡∏ö‡∏≤‡∏á‡∏°‡∏≤‡∏Å‡∏Ñ‡∏∞   \n",
       "1186         ‡πÄ***.                                     ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏õ‡∏Å   \n",
       "140     Pimpida P.                                              ‡∏î‡∏µ‡∏°‡∏≤‡∏Å   \n",
       "...            ...                                                ...   \n",
       "414          T***.                                                 ‡∏î‡∏µ   \n",
       "834        tananya  ‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å‡∏™‡∏±‡πà‡∏á‡∏°‡∏≤‡∏≠‡∏±‡∏ô ‡∏≠‡∏±‡∏ô‡∏ô‡∏∂‡∏á‡πÑ‡∏î‡πâ‡∏¢‡∏¥‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô ‡∏≠‡∏µ‡∏Å‡∏≠‡∏±‡∏ô...   \n",
       "479       WANIDAü§çü¶ã                                 ‡∏Ñ‡∏ô‡∏•‡∏∞‡∏™‡∏µ‡∏Å‡∏∞‡∏ó‡∏µ‡πà‡∏™‡∏±‡πà‡∏á‡πÄ‡∏•‡∏¢   \n",
       "1317         ‡∏à***.   ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏£‡πâ‡∏≤‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å ‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏™‡πà‡∏á‡∏Ç‡∏≠‡∏á‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å‡πÜ ‡πÅ‡∏¢‡πà   \n",
       "802          ‡∏£***.  ‡∏ó‡∏≤‡∏á‡∏£‡πâ‡∏≤‡∏ô‡πÄ‡πÄ‡∏û‡πá‡∏Ñ‡∏Ç‡∏≠‡∏á‡∏°‡∏≤‡∏î‡∏µ‡∏Ñ‡πà‡∏∞ ‡∏Å‡∏≤‡∏£‡∏Ç‡∏ô‡∏™‡πà‡∏á‡∏î‡∏µ ‡∏û‡∏ô‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏™‡πà‡∏á‡∏°‡∏µ...   \n",
       "\n",
       "      Sentiment  \n",
       "217   Neutrally  \n",
       "1419  Neutrally  \n",
       "8      Negative  \n",
       "1186   Negative  \n",
       "140    Positive  \n",
       "...         ...  \n",
       "414   Neutrally  \n",
       "834    Negative  \n",
       "479    Negative  \n",
       "1317   Negative  \n",
       "802    Positive  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df = pd.read_csv(pos_text, delimiter=',', encoding='utf-8').dropna()\n",
    "neg_df = pd.read_csv(neg_text, delimiter=',', encoding='utf-8').dropna()\n",
    "neu_df = pd.read_csv(neu_text, delimiter=',', encoding='utf-8').dropna()\n",
    "\n",
    "data = pd.concat([pos_df, neg_df, neu_df])\n",
    "data.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e54c327-76f2-4ff2-9488-9bc1bec2344a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "Positive     1499\n",
       "Negative     1496\n",
       "Neutrally    1496\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_counts = data['Sentiment'].value_counts()\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f34c495-76c8-4288-ac7a-7072c7e53dd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pythainlp in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (5.0.1)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from pythainlp) (2.31.0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from pythainlp) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from requests>=2.22.0->pythainlp) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from requests>=2.22.0->pythainlp) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from requests>=2.22.0->pythainlp) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from requests>=2.22.0->pythainlp) (2024.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install pythainlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb8ee685-bf13-4535-b5c8-6cfb306931d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5405d397-10f1-4339-8e3c-13093ba995e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Message</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>R***.</td>\n",
       "      <td>‡∏ß‡∏±‡∏™‡∏î‡∏∏ ‡πÅ‡∏Ç‡πá‡∏á ‡πÑ‡∏õ ‡∏´‡∏ô‡πà‡∏≠‡∏¢   ‡∏Ç‡∏ô‡∏≤‡∏î ‡∏û‡∏≠   ‡∏î‡∏µ ‡∏µ   ‡∏Å‡∏≤‡∏£ ‡∏≠‡∏Å‡πÅ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>‡∏£‡∏∏‡πà‡∏á‡∏ô‡∏†‡∏≤</td>\n",
       "      <td>‡πÅ‡∏û‡πá‡∏Å‡πÄ‡∏à ‡πÅ‡∏ô‡πà‡∏´‡∏≤   ‡πÄ‡∏°‡∏∑‡πà‡∏≠ ‡∏°‡∏≤‡∏ñ‡∏∂‡∏á ‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏á ‡πÑ‡∏°‡πà ‡πÄ‡∏Å‡∏¥‡∏î ‡∏Å‡∏≤‡∏£...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>Ploy P.</td>\n",
       "      <td>‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å ‡∏Ñ‡πà‡∏∞   ‡∏™‡∏±‡πà‡∏á ‡∏™‡∏≠‡∏á ‡∏£‡∏≠‡∏ö   ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏± ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠ ‡∏£‡∏≠...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>‡∏≠‡∏£‡∏ô‡∏∏‡∏ä ‡∏†.</td>\n",
       "      <td>‡πÄ‡∏™‡∏∑‡πâ‡∏≠ ‡πÉ‡∏™‡πà ‡∏™‡∏ö‡∏≤‡∏¢ ‡πÑ‡∏°‡πà ‡∏´‡∏ô‡∏≤ ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏ú‡∏≤ ‡∏î‡∏µ ‡∏™‡πà‡∏á ‡πÑ‡∏ß</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>‡∏™‡∏∏‡∏†‡∏≤‡∏†‡∏£‡∏ì‡πå ‡∏™.</td>\n",
       "      <td>‡∏™‡∏±‡πà‡∏á ‡∏™‡∏µ ‡∏î‡∏≥ ‡∏ï‡∏±‡∏ß   ‡πÑ‡∏î‡πâ ‡∏™‡∏µ‡∏ô‡πâ‡∏≥‡πÄ‡∏á‡∏¥ ‡∏Å‡∏±‡∏ö ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß ‡πÄ‡∏Ç‡πâ‡∏° ‡πÜ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Jan S.</td>\n",
       "      <td>‡∏ï‡∏£‡∏á ‡∏ó‡∏µ‡πà ‡πÄ‡∏õ‡πá‡∏ô ‡∏ú‡πâ‡∏≤ ‡∏°‡∏±‡∏ô ‡πÑ‡∏°‡πà‡∏Ñ‡∏≠‡∏¢ ‡∏Å‡∏≤‡∏¢ ‡∏î‡∏≥ ‡∏≠‡πà‡∏∞ ‡∏Ñ‡πà‡∏∞</td>\n",
       "      <td>Neutrally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>Jitlada P.</td>\n",
       "      <td>‡∏™‡∏±‡πà‡∏á ‡∏Å‡∏£‡∏∞‡∏ó ‡πÉ‡∏ö   ‡πÅ‡∏ï‡πà ‡πÑ‡∏°‡πà ‡∏°‡∏µ ‡∏î‡πâ‡∏≤‡∏°‡∏à‡∏±‡∏ö ‡πÄ‡∏•‡∏¢   ‡∏´‡πà‡∏ß‡∏¢ ‡∏™...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>*******601</td>\n",
       "      <td>‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ ‡∏î‡∏µ ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ï‡∏£‡∏á ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á ‡∏ã‡∏∑‡πâ‡∏≠ ‡∏ß‡∏±‡∏™‡∏î‡∏∏...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>‡πÄ‡∏à‡∏ô‡∏à‡∏¥‡∏£‡∏≤ ‡∏à.</td>\n",
       "      <td>‡∏™‡∏±‡πà‡∏á ‡∏ï‡∏±‡∏ß   ‡∏°‡∏µ ‡∏ï‡∏±‡∏ß ‡∏ó‡∏µ‡πà ‡∏ä‡∏≥‡∏£‡∏∏‡∏î ‡∏ñ‡∏∂‡∏á ‡πÄ‡∏õ‡∏ô ‡∏ú‡πâ‡∏≤ ‡∏ñ‡∏π‡∏Å‡πÜ ‡πÅ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1313</th>\n",
       "      <td>‡∏™‡∏≤‡πÇ‡∏£‡∏à‡∏ô‡πå ‡∏¢.</td>\n",
       "      <td>‡∏¢‡∏≠‡∏î‡πÄ‡∏µ‡πà‡∏° ‡∏Å‡∏£‡∏∞‡πÄ‡∏ó‡∏µ‡∏¢‡∏° ‡∏î‡∏≠‡∏á ‡πÅ‡∏ô‡∏∞‡∏≥ ‡∏£‡πâ‡∏≤‡∏ô ‡∏ô‡∏µ‡πâ ‡πÄ‡∏•‡∏¢ ‡∏Ç‡∏≠‡∏á ‡∏î‡∏µ‡∏à...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             User                                            Message  \\\n",
       "19          R***.  ‡∏ß‡∏±‡∏™‡∏î‡∏∏ ‡πÅ‡∏Ç‡πá‡∏á ‡πÑ‡∏õ ‡∏´‡∏ô‡πà‡∏≠‡∏¢   ‡∏Ç‡∏ô‡∏≤‡∏î ‡∏û‡∏≠   ‡∏î‡∏µ ‡∏µ   ‡∏Å‡∏≤‡∏£ ‡∏≠‡∏Å‡πÅ...   \n",
       "54        ‡∏£‡∏∏‡πà‡∏á‡∏ô‡∏†‡∏≤  ‡πÅ‡∏û‡πá‡∏Å‡πÄ‡∏à ‡πÅ‡∏ô‡πà‡∏´‡∏≤   ‡πÄ‡∏°‡∏∑‡πà‡∏≠ ‡∏°‡∏≤‡∏ñ‡∏∂‡∏á ‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏á ‡πÑ‡∏°‡πà ‡πÄ‡∏Å‡∏¥‡∏î ‡∏Å‡∏≤‡∏£...   \n",
       "771       Ploy P.  ‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å ‡∏Ñ‡πà‡∏∞   ‡∏™‡∏±‡πà‡∏á ‡∏™‡∏≠‡∏á ‡∏£‡∏≠‡∏ö   ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏± ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠ ‡∏£‡∏≠...   \n",
       "991      ‡∏≠‡∏£‡∏ô‡∏∏‡∏ä ‡∏†.           ‡πÄ‡∏™‡∏∑‡πâ‡∏≠ ‡πÉ‡∏™‡πà ‡∏™‡∏ö‡∏≤‡∏¢ ‡πÑ‡∏°‡πà ‡∏´‡∏ô‡∏≤ ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏ú‡∏≤ ‡∏î‡∏µ ‡∏™‡πà‡∏á ‡πÑ‡∏ß   \n",
       "651   ‡∏™‡∏∏‡∏†‡∏≤‡∏†‡∏£‡∏ì‡πå ‡∏™.  ‡∏™‡∏±‡πà‡∏á ‡∏™‡∏µ ‡∏î‡∏≥ ‡∏ï‡∏±‡∏ß   ‡πÑ‡∏î‡πâ ‡∏™‡∏µ‡∏ô‡πâ‡∏≥‡πÄ‡∏á‡∏¥ ‡∏Å‡∏±‡∏ö ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß ‡πÄ‡∏Ç‡πâ‡∏° ‡πÜ...   \n",
       "365        Jan S.         ‡∏ï‡∏£‡∏á ‡∏ó‡∏µ‡πà ‡πÄ‡∏õ‡πá‡∏ô ‡∏ú‡πâ‡∏≤ ‡∏°‡∏±‡∏ô ‡πÑ‡∏°‡πà‡∏Ñ‡∏≠‡∏¢ ‡∏Å‡∏≤‡∏¢ ‡∏î‡∏≥ ‡∏≠‡πà‡∏∞ ‡∏Ñ‡πà‡∏∞   \n",
       "1179   Jitlada P.  ‡∏™‡∏±‡πà‡∏á ‡∏Å‡∏£‡∏∞‡∏ó ‡πÉ‡∏ö   ‡πÅ‡∏ï‡πà ‡πÑ‡∏°‡πà ‡∏°‡∏µ ‡∏î‡πâ‡∏≤‡∏°‡∏à‡∏±‡∏ö ‡πÄ‡∏•‡∏¢   ‡∏´‡πà‡∏ß‡∏¢ ‡∏™...   \n",
       "596    *******601  ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ ‡∏î‡∏µ ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ï‡∏£‡∏á ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á ‡∏ã‡∏∑‡πâ‡∏≠ ‡∏ß‡∏±‡∏™‡∏î‡∏∏...   \n",
       "769    ‡πÄ‡∏à‡∏ô‡∏à‡∏¥‡∏£‡∏≤ ‡∏à.  ‡∏™‡∏±‡πà‡∏á ‡∏ï‡∏±‡∏ß   ‡∏°‡∏µ ‡∏ï‡∏±‡∏ß ‡∏ó‡∏µ‡πà ‡∏ä‡∏≥‡∏£‡∏∏‡∏î ‡∏ñ‡∏∂‡∏á ‡πÄ‡∏õ‡∏ô ‡∏ú‡πâ‡∏≤ ‡∏ñ‡∏π‡∏Å‡πÜ ‡πÅ...   \n",
       "1313   ‡∏™‡∏≤‡πÇ‡∏£‡∏à‡∏ô‡πå ‡∏¢.  ‡∏¢‡∏≠‡∏î‡πÄ‡∏µ‡πà‡∏° ‡∏Å‡∏£‡∏∞‡πÄ‡∏ó‡∏µ‡∏¢‡∏° ‡∏î‡∏≠‡∏á ‡πÅ‡∏ô‡∏∞‡∏≥ ‡∏£‡πâ‡∏≤‡∏ô ‡∏ô‡∏µ‡πâ ‡πÄ‡∏•‡∏¢ ‡∏Ç‡∏≠‡∏á ‡∏î‡∏µ‡∏à...   \n",
       "\n",
       "      Sentiment  \n",
       "19     Positive  \n",
       "54     Positive  \n",
       "771    Negative  \n",
       "991    Positive  \n",
       "651    Negative  \n",
       "365   Neutrally  \n",
       "1179   Negative  \n",
       "596    Negative  \n",
       "769    Negative  \n",
       "1313   Positive  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n",
    "def remove_duplicate_chars(text):\n",
    "    if isinstance(text, str):  # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà NaN\n",
    "        unique_words = []\n",
    "        words = word_tokenize(text)\n",
    "        for word in words:\n",
    "            unique_word = ''\n",
    "            for char in word:\n",
    "                if char not in unique_word:\n",
    "                    unique_word += char\n",
    "            unique_words.append(unique_word)\n",
    "        return ' '.join(unique_words)\n",
    "    else:\n",
    "        return text  # ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤ NaN ‡∏Å‡∏•‡∏±‡∏ö‡∏´‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô NaN\n",
    "\n",
    "# ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô `remove_duplicate_chars` ‡πÄ‡∏û‡∏∑‡πà‡∏≠ Tokenize ‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥\n",
    "data['Message'] = data['Message'].apply(remove_duplicate_chars)\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6953b013-4323-40dc-9005-c2a1cbf27f6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels: [2 2 2 ... 1 1 1]\n",
      "Mapping of encoded labels to original labels:\n",
      "0: Negative\n",
      "1: Neutrally\n",
      "2: Positive\n"
     ]
    }
   ],
   "source": [
    "Message = data['Message'].values\n",
    "Sentiment = data['Sentiment'].values\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(Sentiment)\n",
    "print(\"Encoded labels:\", encoded_labels)\n",
    "print(\"Mapping of encoded labels to original labels:\")\n",
    "\n",
    "for label, original_label in enumerate(encoder.classes_):\n",
    "    print(f\"{label}: {original_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ff91332-103d-4ec0-b6cc-3ef648c181de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∏‡∏î‡∏ù‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏ä‡∏∏‡∏î‡∏ó‡∏î‡∏™‡∏≠‡∏ö\n",
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(Message, encoded_labels, stratify=encoded_labels, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1123c15a-6d1d-49a0-a59c-891736ceb4df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        x = self.dropout(pooled_output)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db490af6-a8bb-497a-b5c9-618494d84da8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device).long()  # Convert to Long\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f0f40c5-6fd6-4092-934c-489b65e0d478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ù‡∏∂‡∏Å‡∏™‡∏≠‡∏ô\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            actual_labels.extend(labels.cpu().tolist())\n",
    "    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions, zero_division=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f50117e-93e9-4e8a-ba0f-129938ddfbbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " bert_model_name = 'monsoon-nlp/bert-base-thai'\n",
    " num_classes = 3\n",
    " max_length = 128\n",
    " batch_size = 32\n",
    " num_epochs = 10\n",
    " learning_rate = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2964d90-ffc5-4b1e-960c-631570f47915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#‡∏Ñ‡∏•‡∏≤‡∏™‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e08ebc27-99ba-4dbc-b183-5602a8804a18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á tokenizer ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset ‡πÅ‡∏•‡∏∞ DataLoader ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "train_dataset = TextClassificationDataset(train_sentences, train_labels, tokenizer, max_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = TextClassificationDataset(test_sentences, test_labels, tokenizer, max_length)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9e100a3-9c0e-4d3b-91b4-578398636756",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09326db2-e9c2-4b08-841e-c3d902b7e787",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monsoon-nlp/bert-base-thai were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BERTClassifier(bert_model_name, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29b1da49-9848-4118-9da7-0b2e90ffbca4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î optimizer ‡πÅ‡∏•‡∏∞ scheduler ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• BERT\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a8247b0-a782-490e-ae65-2c8396a9ac14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Validation Accuracy: 0.5800\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.59      0.59       150\n",
      "           1       0.47      0.49      0.48       150\n",
      "           2       0.68      0.66      0.67       150\n",
      "\n",
      "    accuracy                           0.58       450\n",
      "   macro avg       0.58      0.58      0.58       450\n",
      "weighted avg       0.58      0.58      0.58       450\n",
      "\n",
      "Epoch 2/10\n",
      "Validation Accuracy: 0.6156\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.73      0.64       150\n",
      "           1       0.61      0.31      0.41       150\n",
      "           2       0.67      0.81      0.73       150\n",
      "\n",
      "    accuracy                           0.62       450\n",
      "   macro avg       0.62      0.62      0.59       450\n",
      "weighted avg       0.62      0.62      0.59       450\n",
      "\n",
      "Epoch 3/10\n",
      "Validation Accuracy: 0.6311\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.87      0.68       150\n",
      "           1       0.71      0.26      0.38       150\n",
      "           2       0.72      0.77      0.74       150\n",
      "\n",
      "    accuracy                           0.63       450\n",
      "   macro avg       0.66      0.63      0.60       450\n",
      "weighted avg       0.66      0.63      0.60       450\n",
      "\n",
      "Epoch 4/10\n",
      "Validation Accuracy: 0.6733\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.71      0.68       150\n",
      "           1       0.63      0.55      0.59       150\n",
      "           2       0.73      0.76      0.74       150\n",
      "\n",
      "    accuracy                           0.67       450\n",
      "   macro avg       0.67      0.67      0.67       450\n",
      "weighted avg       0.67      0.67      0.67       450\n",
      "\n",
      "Epoch 5/10\n",
      "Validation Accuracy: 0.6267\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.40      0.51       150\n",
      "           1       0.54      0.66      0.59       150\n",
      "           2       0.68      0.82      0.75       150\n",
      "\n",
      "    accuracy                           0.63       450\n",
      "   macro avg       0.64      0.63      0.62       450\n",
      "weighted avg       0.64      0.63      0.62       450\n",
      "\n",
      "Epoch 6/10\n",
      "Validation Accuracy: 0.6489\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.44      0.55       150\n",
      "           1       0.58      0.63      0.60       150\n",
      "           2       0.67      0.87      0.76       150\n",
      "\n",
      "    accuracy                           0.65       450\n",
      "   macro avg       0.66      0.65      0.64       450\n",
      "weighted avg       0.66      0.65      0.64       450\n",
      "\n",
      "Epoch 7/10\n",
      "Validation Accuracy: 0.6756\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.77      0.68       150\n",
      "           1       0.68      0.48      0.56       150\n",
      "           2       0.75      0.77      0.76       150\n",
      "\n",
      "    accuracy                           0.68       450\n",
      "   macro avg       0.68      0.68      0.67       450\n",
      "weighted avg       0.68      0.68      0.67       450\n",
      "\n",
      "Epoch 8/10\n",
      "Validation Accuracy: 0.6756\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.57      0.64       150\n",
      "           1       0.65      0.59      0.62       150\n",
      "           2       0.67      0.87      0.76       150\n",
      "\n",
      "    accuracy                           0.68       450\n",
      "   macro avg       0.68      0.68      0.67       450\n",
      "weighted avg       0.68      0.68      0.67       450\n",
      "\n",
      "Epoch 9/10\n",
      "Validation Accuracy: 0.6978\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.68      0.68       150\n",
      "           1       0.61      0.69      0.65       150\n",
      "           2       0.81      0.73      0.77       150\n",
      "\n",
      "    accuracy                           0.70       450\n",
      "   macro avg       0.71      0.70      0.70       450\n",
      "weighted avg       0.71      0.70      0.70       450\n",
      "\n",
      "Epoch 10/10\n",
      "Validation Accuracy: 0.7089\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.66      0.68       150\n",
      "           1       0.66      0.65      0.65       150\n",
      "           2       0.76      0.82      0.79       150\n",
      "\n",
      "    accuracy                           0.71       450\n",
      "   macro avg       0.71      0.71      0.71       450\n",
      "weighted avg       0.71      0.71      0.71       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        train(model, train_dataloader, optimizer, scheduler, device)\n",
    "        accuracy, report = evaluate(model, val_dataloader, device)\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e1867cb-0b94-48b6-9252-0308908a2323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"C:/Users/Gumpun/Sentiment-Analysis-System-for-Consumer-Products/bert_classifier.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c8e5985f-7218-4a42-80af-cd4bf0b764d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('C:/Users/Gumpun/Sentiment-Analysis-System-for-Consumer-Products/bert_classifier.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "073c416f-c681-4575-b672-c1a6ab876849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer, device, max_length=128):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        \n",
    "    sentiment_mapping = {0: \"Negative\", 1: \"Neutrally\", 2: \"Positive\"}\n",
    "    return sentiment_mapping[preds.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9f95cf16-a4fb-4cb3-aba5-b57cab1957e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "test_text = remove_duplicate_chars(\"‡∏Ñ‡∏ô‡∏•‡∏∞‡∏™‡∏µ‡∏Å‡∏∞‡∏ó‡∏µ‡πà‡∏™‡∏±‡πà‡∏á‡πÄ‡∏•‡∏¢\")\n",
    "sentiment = predict_sentiment(test_text, model, tokenizer, device)\n",
    "print(f\"Predicted sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5d149ac8-1039-4bb0-8fed-217f74de94a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‡∏Ñ‡∏ô‡∏•‡∏∞', '‡∏™‡∏µ', '‡∏Å‡∏∞', '‡∏ó‡∏µ‡πà', '‡∏™‡∏±‡πà‡∏á', '‡πÄ‡∏•‡∏¢']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = word_tokenize(remove_duplicate_chars(test_text), keep_whitespace=False)\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b2b18b75-767e-4915-a515-38bc0356b50a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡∏Ñ‡∏ô‡∏•‡∏∞\n",
      "Predicted sentiment: Neutrally\n",
      "----------------------\n",
      "‡∏™‡∏µ\n",
      "Predicted sentiment: Neutrally\n",
      "----------------------\n",
      "‡∏Å‡∏∞\n",
      "Predicted sentiment: Negative\n",
      "----------------------\n",
      "‡∏ó‡∏µ‡πà\n",
      "Predicted sentiment: Positive\n",
      "----------------------\n",
      "‡∏™‡∏±‡πà‡∏á\n",
      "Predicted sentiment: Negative\n",
      "----------------------\n",
      "‡πÄ‡∏•‡∏¢\n",
      "Predicted sentiment: Negative\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "for i in tokenized_text:\n",
    "    print(i)\n",
    "    test_text = remove_duplicate_chars(i)\n",
    "    sentiment = predict_sentiment(test_text, model, tokenizer, device)\n",
    "    print(f\"Predicted sentiment: {sentiment}\")\n",
    "    print(\"----------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
