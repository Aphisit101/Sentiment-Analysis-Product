{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9154ddd4-1fb5-4482-afd3-907dc72dcd70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gumpun\\AppData\\Local\\Temp\\ipykernel_8820\\2203224730.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "138215fe-9eed-4df7-9ca9-ce721bafb540",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: attacut in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (1.0.6)\n",
      "Requirement already satisfied: docopt>=0.6.2 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from attacut) (0.6.2)\n",
      "Requirement already satisfied: fire>=0.1.3 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from attacut) (0.5.0)\n",
      "Requirement already satisfied: nptyping>=0.2.0 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from attacut) (2.5.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from attacut) (1.26.3)\n",
      "Requirement already satisfied: pyyaml>=5.1.2 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from attacut) (6.0.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from attacut) (1.16.0)\n",
      "Requirement already satisfied: ssg>=0.0.4 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from attacut) (0.0.8)\n",
      "Requirement already satisfied: torch>=1.2.0 in c:\\users\\gumpun\\appdata\\roaming\\python\\python310\\site-packages (from attacut) (2.2.0+cu118)\n",
      "Requirement already satisfied: termcolor in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from fire>=0.1.3->attacut) (2.4.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.6 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from ssg>=0.0.4->attacut) (0.9.10)\n",
      "Requirement already satisfied: tqdm>=4.32.2 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from ssg>=0.0.4->attacut) (4.66.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from torch>=1.2.0->attacut) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from torch>=1.2.0->attacut) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from torch>=1.2.0->attacut) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from torch>=1.2.0->attacut) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from torch>=1.2.0->attacut) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from torch>=1.2.0->attacut) (2024.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from tqdm>=4.32.2->ssg>=0.0.4->attacut) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from jinja2->torch>=1.2.0->attacut) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from sympy->torch>=1.2.0->attacut) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install attacut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86b73746-586a-4032-a9c8-ca3c541a3996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pythainlp in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (5.0.1)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from pythainlp) (2.31.0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from pythainlp) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from requests>=2.22.0->pythainlp) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from requests>=2.22.0->pythainlp) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from requests>=2.22.0->pythainlp) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from requests>=2.22.0->pythainlp) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install pythainlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8e05723-b53c-4516-a43b-876b6f2f8bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos_text = (\"C:/Users/Gumpun/Sentiment-Analysis-System-for-Consumer-Products/DataReviewProductThai/Positive.csv\")\n",
    "neg_text = (\"C:/Users/Gumpun/Sentiment-Analysis-System-for-Consumer-Products/DataReviewProductThai/Negative.csv\")\n",
    "neu_text = (\"C:/Users/Gumpun/Sentiment-Analysis-System-for-Consumer-Products/DataReviewProductThai/Neutrally.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78575318-4d32-4b25-8e37-2812ec9fed4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Message</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>ร***.</td>\n",
       "      <td>มาไม่ตรกปกไม่ชอบไม่ผ่านไม่อย่างสั่งแล้ว</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Penpisut S.</td>\n",
       "      <td>ส่งสีเสื้อมาไม่ตรงกับที่สั่ง</td>\n",
       "      <td>Neutrally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>0***0</td>\n",
       "      <td>วัสดุ ขนาด การออกแบบ ดีคะดีมากกกกกกกกกกกกกกกกก...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217</th>\n",
       "      <td>Yui J.</td>\n",
       "      <td>ผ้านิ่มมาก แต่บาง เหมาะกับประเทศไทย</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>samreong N.</td>\n",
       "      <td>คุณภาพผ้าพอใช้แต่สีผ้าดูจะสีซีไป</td>\n",
       "      <td>Neutrally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>สุนันท์ ค.</td>\n",
       "      <td>สั่งสีครีมได้สีม่วงเฉ๊ย</td>\n",
       "      <td>Neutrally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>ฐิรนันทร์ ร.</td>\n",
       "      <td>ผ้าเบาใส่สบาย</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411</th>\n",
       "      <td>โภคิน</td>\n",
       "      <td>โฟมซักแห้งรองเท้าสีขาวใช้ได้ดีในระดับหนึ่งครับ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>*******038</td>\n",
       "      <td>เสื้อบางมากกก</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Chp I.</td>\n",
       "      <td>ราคาโดนใจ ผ้าพอได้ใข้ ส่งเร็วแต่อยากได้สีเข้มๆ</td>\n",
       "      <td>Neutrally</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              User                                            Message  \\\n",
       "1447         ร***.            มาไม่ตรกปกไม่ชอบไม่ผ่านไม่อย่างสั่งแล้ว   \n",
       "227    Penpisut S.                       ส่งสีเสื้อมาไม่ตรงกับที่สั่ง   \n",
       "799          0***0  วัสดุ ขนาด การออกแบบ ดีคะดีมากกกกกกกกกกกกกกกกก...   \n",
       "1217        Yui J.                ผ้านิ่มมาก แต่บาง เหมาะกับประเทศไทย   \n",
       "1461   samreong N.                   คุณภาพผ้าพอใช้แต่สีผ้าดูจะสีซีไป   \n",
       "...            ...                                                ...   \n",
       "40      สุนันท์ ค.                            สั่งสีครีมได้สีม่วงเฉ๊ย   \n",
       "1047  ฐิรนันทร์ ร.                                      ผ้าเบาใส่สบาย   \n",
       "1411         โภคิน  โฟมซักแห้งรองเท้าสีขาวใช้ได้ดีในระดับหนึ่งครับ...   \n",
       "399     *******038                                      เสื้อบางมากกก   \n",
       "114         Chp I.     ราคาโดนใจ ผ้าพอได้ใข้ ส่งเร็วแต่อยากได้สีเข้มๆ   \n",
       "\n",
       "      Sentiment  \n",
       "1447   Negative  \n",
       "227   Neutrally  \n",
       "799    Positive  \n",
       "1217   Positive  \n",
       "1461  Neutrally  \n",
       "...         ...  \n",
       "40    Neutrally  \n",
       "1047   Positive  \n",
       "1411   Positive  \n",
       "399    Positive  \n",
       "114   Neutrally  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df = pd.read_csv(pos_text, delimiter=',', encoding='utf-8').dropna()\n",
    "neg_df = pd.read_csv(neg_text, delimiter=',', encoding='utf-8').dropna()\n",
    "neu_df = pd.read_csv(neu_text, delimiter=',', encoding='utf-8').dropna()\n",
    "\n",
    "data = pd.concat([pos_df, neg_df, neu_df])\n",
    "data.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d5da3bc-150c-4ad4-a9d0-977c270f5b79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "Positive     1499\n",
       "Negative     1496\n",
       "Neutrally    1496\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_counts = data['Sentiment'].value_counts()\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "201b5e1c-d7e5-43f5-b809-dbcec6caaddc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pythainlp in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (5.0.1)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from pythainlp) (2.31.0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from pythainlp) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from requests>=2.22.0->pythainlp) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from requests>=2.22.0->pythainlp) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from requests>=2.22.0->pythainlp) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from requests>=2.22.0->pythainlp) (2024.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install pythainlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0714194e-fc0f-4afd-9f4c-03c2b2799839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pythainlp.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "711669aa-9476-4e4f-b0e7-e813df8e4fa1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Message</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>aiyarin</td>\n",
       "      <td>แย่มาก เลย คะ   ส่ง กางเ ขาด มา ขาย ให้ ตัว   ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>พนมพร</td>\n",
       "      <td>ไม่ นุ่ม อย่าง ที่ คิด เลย จ้า   พื้น แข็ง มา ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>ช***า</td>\n",
       "      <td>สั่ง สี ดำ   ได้ สี กรม กับ ส้ม อิฐ   ส่วน สี ...</td>\n",
       "      <td>Neutrally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>*******408</td>\n",
       "      <td>คุ้ม่า กับ รา คาที่ ดี   เหมาะสำรับ สวมใ่ ใน ช...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>0***2</td>\n",
       "      <td>เวลา เอา ผ้า ไป ปั่น   เนื้อผา   ยุ่ มาก   หลุ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>รังรักษ์ ส.</td>\n",
       "      <td>ส่ง ไว วส   ผ้า บา ง ง ง ง ง ง ง   คุณภาพ ตาม ...</td>\n",
       "      <td>Neutrally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>ชาติยา ล.</td>\n",
       "      <td>การเคลื่อนไหว ที่ เงียบส และ ราบื่น   ใส่ ถ่าน...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>จิรฐา ว.</td>\n",
       "      <td>รูปแบ สวยงาม   ดู สะอาด ตา ดู ง่าย ดี ค่ะ ชอบ ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Nu_nuy</td>\n",
       "      <td>สินค้า ไม่ โอเค ด้าย เย็บ หลุด อก</td>\n",
       "      <td>Neutrally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>จารุณี</td>\n",
       "      <td>ได้รับ สินค้า แล้ว ค่ะ   สั่งซื้อ กางเ กีฬา ขา...</td>\n",
       "      <td>Neutrally</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             User                                            Message  \\\n",
       "1336      aiyarin  แย่มาก เลย คะ   ส่ง กางเ ขาด มา ขาย ให้ ตัว   ...   \n",
       "1024        พนมพร  ไม่ นุ่ม อย่าง ที่ คิด เลย จ้า   พื้น แข็ง มา ...   \n",
       "1451        ช***า  สั่ง สี ดำ   ได้ สี กรม กับ ส้ม อิฐ   ส่วน สี ...   \n",
       "554    *******408  คุ้ม่า กับ รา คาที่ ดี   เหมาะสำรับ สวมใ่ ใน ช...   \n",
       "842         0***2  เวลา เอา ผ้า ไป ปั่น   เนื้อผา   ยุ่ มาก   หลุ...   \n",
       "1406  รังรักษ์ ส.  ส่ง ไว วส   ผ้า บา ง ง ง ง ง ง ง   คุณภาพ ตาม ...   \n",
       "1022    ชาติยา ล.  การเคลื่อนไหว ที่ เงียบส และ ราบื่น   ใส่ ถ่าน...   \n",
       "282      จิรฐา ว.  รูปแบ สวยงาม   ดู สะอาด ตา ดู ง่าย ดี ค่ะ ชอบ ...   \n",
       "90         Nu_nuy                  สินค้า ไม่ โอเค ด้าย เย็บ หลุด อก   \n",
       "1154       จารุณี  ได้รับ สินค้า แล้ว ค่ะ   สั่งซื้อ กางเ กีฬา ขา...   \n",
       "\n",
       "      Sentiment  \n",
       "1336   Negative  \n",
       "1024   Positive  \n",
       "1451  Neutrally  \n",
       "554    Positive  \n",
       "842    Negative  \n",
       "1406  Neutrally  \n",
       "1022   Negative  \n",
       "282    Positive  \n",
       "90    Neutrally  \n",
       "1154  Neutrally  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# สร้างฟังก์ชันสำหรับการลบอักขระที่ซ้ำกันในแต่ละคำเท่านั้น\n",
    "def remove_duplicate_chars(text):\n",
    "    if isinstance(text, str):  # ตรวจสอบว่าข้อความไม่ใช่ NaN\n",
    "        unique_words = []\n",
    "        words = word_tokenize(text)\n",
    "        for word in words:\n",
    "            unique_word = ''\n",
    "            for char in word:\n",
    "                if char not in unique_word:\n",
    "                    unique_word += char\n",
    "            unique_words.append(unique_word)\n",
    "        return ' '.join(unique_words)\n",
    "    else:\n",
    "        return text  # ส่งค่า NaN กลับหากเป็น NaN\n",
    "\n",
    "# ใช้ฟังก์ชัน `remove_duplicate_chars` เพื่อ Tokenize และลบคำที่ซ้ำ\n",
    "data['Message'] = data['Message'].apply(remove_duplicate_chars)\n",
    "data.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d07d6053-e83d-42d2-aa08-195832a8faa0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                            รองเท้า นุ่ม\n",
       "1                                              คุณภาพดี ๆ\n",
       "2       จัดส่ง รวดเ็   ห่อ แพ ค สินค้า อย่าง ดี   บี เ...\n",
       "3       รูปลักษณ์ภายนอ ดู ดีมาก   แต่ คุณภาพ แย่มาก ๆ ...\n",
       "4       ส่ง เร็ว ตรง ตามกำหนด   วัน   สินค้า ครบ   ถูก...\n",
       "                              ...                        \n",
       "1495                                       กระท มี รอย บุ\n",
       "1496    เนื้อผา ค่อนข้าง บาง   แต่ ลาย สวยงาม   ตรง ตา...\n",
       "1497                       เ็ยบ เข็ม เดียว   ขาด ง่าย มาก\n",
       "1498    มี ให้ เลือก สี หลากย   ผ้า เบา สบาย ใน การเคล...\n",
       "1499    ผ้า สวย ทรง สวย แต่ เสียดา ไซส์ น้อย ไป หน่อย ...\n",
       "Name: Message, Length: 4491, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3a13f4c9-be43-4c8e-96df-5280ac755fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels: [2 2 2 ... 1 1 1]\n",
      "Mapping of encoded labels to original labels:\n",
      "0: Negative\n",
      "1: Neutrally\n",
      "2: Positive\n"
     ]
    }
   ],
   "source": [
    "Message = data['Message'].values\n",
    "Sentiment = data['Sentiment'].values\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(Sentiment)\n",
    "print(\"Encoded labels:\", encoded_labels)\n",
    "print(\"Mapping of encoded labels to original labels:\")\n",
    "\n",
    "for label, original_label in enumerate(encoder.classes_):\n",
    "    print(f\"{label}: {original_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6a941c16-f167-485e-89ff-7eeb34d169f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# แบ่งข้อมูลเป็นชุดฝึกและชุดทดสอบ\n",
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(Message, encoded_labels, stratify=encoded_labels, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0e53370f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gumpun\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "100%|██████████| 4041/4041 [00:07<00:00, 539.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 1942 unique words\n",
      "Label 1: 1743 unique words\n",
      "Label 2: 1703 unique words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#โค้ดด้านบนน่าจะใช้สำหรับการหาความถี่ของคำที่เป็นเอกลักษณ์สำหรับแต่ละป้ายกำกับในชุดข้อมูลการจำแนกประเภทข้อความ\n",
    "\n",
    "unique_labels = list(set(train_labels))\n",
    "label_mapping = {label: i for i, label in enumerate(unique_labels)}\n",
    "vec = CountVectorizer(max_features=5000, tokenizer=word_tokenize)\n",
    "X = vec.fit_transform(train_sentences)\n",
    "vocab = vec.get_feature_names_out()\n",
    "X = X.toarray()\n",
    "word_counts = {}\n",
    "for label in unique_labels:\n",
    "    word_counts[label] = defaultdict(lambda: 0)\n",
    "for i in tqdm(range(X.shape[0])):\n",
    "    label = label_mapping[train_labels[i]]\n",
    "    words_in_sentence = word_tokenize(train_sentences[i])\n",
    "    for word in words_in_sentence:\n",
    "        if word in vocab:\n",
    "            word_counts[label][word] += 1\n",
    "for label, counts in word_counts.items():\n",
    "    print(f\"Label {label}: {len(counts)} unique words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "480f4b8e-fb74-41bf-a5ad-a876225d8fe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ฟังก์ชันนี้ใช้สำหรับการทำนายความน่าจะเป็นของคำในเอกสารที่กำลังจะถูกจำแนก\n",
    "def laplace_smoothing(n_label_items, vocab, word_counts, word, text_label):\n",
    "    a = word_counts[text_label][word] + 1\n",
    "    b = n_label_items[text_label] + len(vocab)\n",
    "    return math.log(a/b)\n",
    "\n",
    "#ฟังก์ชันนี้ใช้เพื่อแบ่งข้อมูลเป็นกลุ่มตามป้ายกำกับที่ให้มา\n",
    "def group_by_label(x, y, labels):\n",
    "    data = {}\n",
    "    for l in labels:\n",
    "        data[l] = x[np.where(y == l)]\n",
    "\n",
    "    #print(data)\n",
    "    return data\n",
    "\n",
    "def fit(x, y, labels):\n",
    "    n_label_items = {}\n",
    "    log_label_priors = {}\n",
    "    n = len(x)\n",
    "    grouped_data = group_by_label(x, y, labels)\n",
    "    for l, data in grouped_data.items():\n",
    "\n",
    "        n_label_items[l] = len(data)\n",
    "        log_label_priors[l] = math.log(n_label_items[l] / n)\n",
    "    return n_label_items, log_label_priors\n",
    "\n",
    "def predict(n_label_items, vocab, word_counts, log_label_priors, labels, x):\n",
    "    result = []\n",
    "    for text in x:\n",
    "        label_scores = {l: log_label_priors[l] for l in labels}\n",
    "\n",
    "        words = set(text.strip().split(\" \")) #split text with space\n",
    "        #print(words)\n",
    "        for word in words:\n",
    "            if word not in vocab: continue\n",
    "            for l in labels:\n",
    "                log_w_given_l = laplace_smoothing(n_label_items, vocab, word_counts, word, l)\n",
    "                label_scores[l] += log_w_given_l\n",
    "        #print(max(label_scores, key=label_scores.get))\n",
    "        result.append(max(label_scores, key=label_scores.get))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "02e0359b-812a-4dd9-bcf0-3f724b0de823",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = [0,1,2]\n",
    "n_label_items, log_label_priors = fit(train_sentences, train_labels, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "64750701-faf1-49a3-b033-fed0f298f9a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediction on test set :  0.6444444444444445\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "pred = predict(n_label_items, vocab, word_counts, log_label_priors, labels, test_sentences)\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_labels, pred)\n",
    "print(\"Accuracy of prediction on test set : \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f6eff48d-5057-4caf-944d-cfe81b7c03db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.66      0.66       150\n",
      "           1       0.79      0.33      0.46       150\n",
      "           2       0.60      0.95      0.74       150\n",
      "\n",
      "    accuracy                           0.64       450\n",
      "   macro avg       0.68      0.64      0.62       450\n",
      "weighted avg       0.68      0.64      0.62       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9efcde02-0d4f-422e-b611-a29795e37a63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is classified as neutrally.\n"
     ]
    }
   ],
   "source": [
    "test_text = remove_duplicate_chars(\"ดีค่ะแต่บางไปหน่อย\")\n",
    "predicted_label = predict(n_label_items, vocab, word_counts, log_label_priors, labels, [test_text])\n",
    "\n",
    "\"\"\"\n",
    "0: Negative\n",
    "1: Neutrally\n",
    "2: Positive\n",
    "\"\"\"\n",
    "\n",
    "if predicted_label[0] == 0:\n",
    "    print(\"The text is classified as negative.\")\n",
    "elif predicted_label[0] == 1:\n",
    "    print(\"The text is classified as neutrally.\")\n",
    "else:\n",
    "    print(\"The text is classified as positive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "54a2879b-e76d-43a9-8715-185dde9821f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ดี\n",
      "Predicted sentiment: 2\n",
      "----------------------\n",
      "Text: ค่ะ\n",
      "Predicted sentiment: 2\n",
      "----------------------\n",
      "Text: แต่\n",
      "Predicted sentiment: 1\n",
      "----------------------\n",
      "Text: บาง\n",
      "Predicted sentiment: 1\n",
      "----------------------\n",
      "Text: ไป\n",
      "Predicted sentiment: 1\n",
      "----------------------\n",
      "Text: หน่อย\n",
      "Predicted sentiment: 1\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = test_text.split()\n",
    "for word in tokenized_text:\n",
    "    sentiment = predict(n_label_items, vocab, word_counts, log_label_priors, labels, [word])\n",
    "    print(f\"Text: {word}\")\n",
    "    print(f\"Predicted sentiment: {sentiment[0]}\")\n",
    "    print(\"----------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
